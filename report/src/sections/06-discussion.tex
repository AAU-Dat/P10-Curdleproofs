

\section{Discussion}\label{sec:discussion}
The following section will discuss the results from the experiment.
One of the purposes of the paper is
to reproduce the attack mentioned in~\cite{heimbach2024deanonymizingethereumvalidatorsp2p}.
Therefore, this section compares these results against each other, exploring similarities and differences.


As our experiment handles data that could be coupled to individuals,
we also explore ethical considerations regarding our work.


\subsection{Ethical considerations}\label{subsec:ethical-considerations}
As the paper tackles an attack on the Ethereum network,
it is important to consider the ethical implications of the work.

These implications are mainly related to the potential harm that could be caused by the attack.
Because of the potential harm, we have decided to not disclose the exact implementation and details of the attack,
and limited the information to what is necessary to understand the attack and its implications.
This is to prevent any malicious actors from easily replicating the attack and causing harm to the mainnet.
This is also why the GitHub repository containing the code for the attack is private.

These considerations are also the reason why we only ran the attack on a testnet and not on the mainnet.
Running the attack on the testnet has some benefits in the form
of not having to worry about the loss of any real ether.
There is also a greater number of active validators to reach compared to the mainnet.

Though, it also has some drawbacks.
The main drawback is that the testnet does not have the same restrictions for entering a validator as in the mainnet,
which could affect the results of the attack.
Since one does not have to put any money in to the system to become a validator,
the density of irregular validators will be higher.
This could affect the results of the attack compared to the original paper, which was run on the mainnet.

Even though the attack is run on a testnet where the money in the system is not a problem.
The attack could still have some negative consequences.
Since the attack seeks to deanonymize validators by gaining access to the IPs of nodes in the system, it could lead to a loss of privacy.
This is an issue that was important to consider when collecting data for the attack.

The data that was collected was stored in a database and was only used for the purpose of analysing the attack.

\subsection{Comparison of results}\label{subsec:res-comparison}

\subsubsection{Connected peers}\label{subsubsec:connected-peers}
As we can see in~\autoref{fig:peersconnected}, the number of connected peers fluctuated a lot over the course of the experiment.
It also has a similar pattern to the one in the original paper when it comes to the sudden drops in the number of connected peers in set intervals.
In the original paper, they also experienced sudden drops in the number of connected peers, and they are not sure why this happens.
They suspect that it could be due to some artifacts from AWS, however we too experienced these drops.
In our case we are not running on AWS, but on Strato CLAUUDIA, which is a cloud computing service made available by Aalborg University.
So either CLAAUDIA has the same artifacts as AWS, or the drops are due to some other reason.
We suspect that the drops could be due to garbage collection of peers that have been idle for too long.
Each node calculates a peer score for each of their peers to tell whether it should stay connected to that node or not.
Therefore, the drop of peers could be a result of how other peers calculated our node's peer score.
If something made our node get a bad peer score the number of connected peers to our node could drop.
The reason that our node could get a bad peer score could relate to performance issues.
We can see in~\autoref{fig:peersconnected} that drops happen when around 250 peers were connected.
It could be that logging attestation from this amount peers hit a bottleneck for our node, making it slow down and hence receive a bad peer score.

\subsubsection{De-anonymization}\label{subsubsec:de-anonymization}
When it comes to the destination of nodes into the four different categories, we see that it differs in some of the areas.
In the~\gls{de-anon paper} two of the four nodes show the same de-anonymization rate as we found in our experiment.
These are the same two nodes that also share the same peer connection amount as have.
The SO node show especially similar results to our node also having a close rest rate to ours.
When it comes to the no validators category, we see that the~\gls{de-anon paper} has a higher rate of nodes that have no validators on their SO and FR nodes, but similar one on VA and ZH.
The 64 subnets category is the one that differs the most between the two experiments.
The~\gls{de-anon paper} has a rate of nodes, that are subscribed to all 64 subnets, near 9 times smaller than the one we find.


There could be a few reasons for these difference between the results.
The main one being the type of network that the attack was run on.
Since we ran the attack on the Holesky testnet, the behavior of the nodes are different from what it would be if they had been on the mainnet.
We think at this is why we se an increase in the amount of nodes that deviating from the default two subnets and are subscribed to all 64 subnets.
We also think that this could be the reason for the higher rate of nodes in the rest category.
Since more nodes would be turned on and off and switch IPs more often.


This is also why think that the amount of non-uniques validators is higher.
Because even though we found more validators than the~\gls{de-anon paper} and de-anonymized a larger portion of the network, we found a higher rate of non-unique validators comparatively.
We attribute this to the large amount of nodes that inhabit some of the nodes that we connected to.



\subsubsection{Validator distribution}\label{subsubsec:validator-distribution}
While comparing the results from the original paper to our results, we see that the distribution of validators on peers is a lot different.
In the original paper, the amount of validators on each peer tops out at a little over 1000 validators per peer.
Our finding however, shows that the amount of validators on each peer could reach up to 87028 validators per peer.
One of the explanations for this could be that the testnet has a higher density of validators compared to the mainnet since it is easier to become a validator on the testnet due to money not being a problem.
In the original paper they also state that 27\% of the peers they connected to had only one validator.
Opposite to this, we found that only 7\% of the peers we connected to had just one validator.
