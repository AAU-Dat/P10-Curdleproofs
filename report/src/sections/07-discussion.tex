

\section{Discussion}\label{sec:discussion}
In this section, we will discuss the results of the experiments in \autoref{sec:results} and how they relate to the CAAUrdleproofs protocol.
We will also discuss some of the limitations of the CAAUrdleproofs protocol and how it compares to Curdleproofs.


\subsection{CAAUrdleproofs in Comparison to Curdleproofs}\label{subsec:CAAUrdleproofs-vs-Curdleproofs}
As mentioned in~\autoref{subsec:results:provingverifying}, the proving and verifying times between the two protocols are close to identical when $\ell$ is a power of two.
This is because the added computation is negligible compared to the other computations present in the original Curdleproofs protocol.

On the prover, there is the addition of the scheme function from Springproofs.
However, as seen in~\autoref{lst:schemefunc}, the scheme function only performs integer calculations based on $n$ and hence should have a negligible impact compared to the cryptographic group computations.
Additionally, as mentioned in~\autoref{subsec:approach-implementation}, the vector is never practically split in two; instead, it uses pointers.
Therefore, we avoid having to add new variables to memory in every round.

Also, we mentioned in~\autoref{subsec:approach-implementation} that every round after the first runs the same code as Curdleproofs.
Thus, only the first round should be able to introduce some computational overhead.
However, as mentioned before, the overhead should be negligible.

The same kind of explanation can be used to describe the same scenario at powers of two on the verifier side.
Looking at~\autoref{lst:ipa-verifier-optimized}, we see that the only difference in computation between CAAUrdleproofs and Curdleproofs stems from the calculation of $\mathbf{s}$.
Comparing line 29 and lines 33--47, it becomes clear that both ways work in $\mathcal{O}(n\log n)$, as they do $m$ computations for each of the $n$ elements.
CAAUrdleproofs does, however, require additional integer variables for splitting, as well as an array to keep track of the active positions of the vector elements during recursion.
Nevertheless,~\autoref{fig:resulttimes}(b) shows that this does not have a big, if any, impact on the running time.

However, as mentioned in~\autoref{subsec:results:provingverifying}, when~$\ell$ is just above a power of two, we observe some more aggressively increasing verification times.
We expect this to be a result of~$m$ being set to~$\lceil\log n\rceil$ in line 8 of~\autoref{lst:ipa-verifier-optimized}.
For, when~$\ell$ is~$65$, computations for an additional round are added, compared to when~$\ell$ is~$64$.
This explains why the running time flattens when~$\ell$ is increased.
The pattern shows that the bump has a decreasing impact on the running time as~$\ell$ increases.
In theory, the extra recursive round should introduce a constant amount of work for the verifier.
Therefore, we believe the bump to be an artifact of memory optimizations.
For instance, pre-fetching, where the memory system can optimize access if it suspects some values in memory are going to be reused\footnote{\href{https://doc.rust-lang.org/core/arch/aarch64/fn._prefetch.html}{https://doc.rust-lang.org/core/arch/aarc.h64/fn.\_prefetch.html} â€” Accessed: 29/05/2025}.
As~$\ell$ increases, the memory system will have more data to predict and optimize memory access.

\subsection{Shuffle Security}\label{subsec:Discution-Shuffle-security}
When looking at the results of the shuffle security experiment in \autoref{fig:shufflesecurity} and \autoref{fig:shufflesecurityviolin}, we can see that when taking into account the standard deviation, the shuffle can still be secure with an~$\ell$ as low as 32 within the 8,192 shuffles available.
Even when taking into account the worst-case scenario from our experiment, the shuffle will still be secure with an~$\ell$ as low as 42 within the 8,192 shuffles available with an $\alpha$ of 8,192.

We do not recommend using an~$\ell$ lower than 80, as in this case, the worst-case scenario requires a little under half of the available shuffles to be honest, in order to be secure.
As seen in~\autoref{fig:shufflesecurity}, the protocol would also only need a third of the 8,192 shuffles to be honest to get within the standard deviation.
Lowering the shuffle size to 80 would still lead to a reduction of the proving time of 62.69 ms, which is 74.25\% of the current Curdleproofs time, and a reduction in the verifying time of 0.89 ms, which is 96.11\% of the current Curdleproofs time.
It would also reduce the block overhead size from 16.656 KB to 12.048 KB\@.
The reduced size is only 72.33\% of the current size for Curdleproofs, which would result in saving $\sim12.11$ GB of space on the blockchain each year.
Some other factors to consider when determining the number of honest shuffles required to secure the shuffle are that there are additional elements that can impact the blockchain's security.
One such element is the known attacks that exploit the ability to control a large number of validators.
Attacks like the $\geq50\%$ stake attack and the $33\%$ finality attack~\cite{EthereumAttackDefense2024} take advantage of controlling a large number of validators in order to affect the blockchain system negatively.
Because of attacks like these, which rely on controlling a large number of validators, we recommend that when evaluating how many honest shuffles are necessary to make the shuffle secure, one should also consider how many honest validators are required to secure the blockchain.

Another thing to keep in mind is that within the Ethereum system, not every validator is owned by a different person.
Some nodes contain multiple validators, and this means that during the shuffling phase when selecting the 16,384 possible proposers, there is a chance that a single node controls multiple of the chosen validators.
Likewise, it is also possible during the selection of the shufflers.

From the results in~\autoref{fig:shufflesecurity}, we see that the mean starts higher and ends lower for the experiments with a higher $\alpha$.
One reason for this could be the relationship between the number of adversarial tracked cups and the threshold required before the shuffle is secure.
Since the threshold is $2/(n-\alpha)$, the higher $\alpha$ is, the higher the threshold for the amount of water allowed in any cup, see~\autoref{subsec:approach-implementation}.
Therefore, the higher $\alpha$ is, the harder it is to get the water divided into the honest cups.
The reason is that the distribution only happens in honest cups.
More adversarial cups means less honest cups to distribute the water into.
Hence, there potentially is a higher amount of water in the chosen cups after a shuffle when~$\alpha$ is higher.
