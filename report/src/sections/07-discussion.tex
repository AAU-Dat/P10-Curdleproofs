

\section{Discussion}\label{sec:discussion}
In this section we will discuss the results of the experiments in \autoref{sec:results} and how they relate to the CAAUrdleproofs protocol.
We will also discuss some of the limitations of the CAAUrdleproofs protocol and how it compares to Curdleproofs.


\subsection{CAAUrdleproofs in comparison to Curdleproofs}\label{subsec:CAAUrdleproofs-vs-Curdleproofs}
As mentioned in~\autoref{subsec:results:provingverifying}, the proving and verifying times between the two protocols are close to identical when $\ell$ is a power of two.
We expect that this is because the added computation is negligible compared to other computations that are present in the original Curdleproofs protocol.

On the prover, there is the addition of the scheme function from Springproofs.
Though, as seen in~\autoref{lst:schemefunc}, the scheme function only makes integer calculations based on $n$, and hence should have a negligible impact compared to the cryptographic group computations.
In addition to that, mentioned in~\autoref{subsec:approach-implementation}, the vector is never practically split in two but instead uses pointers.
Therefore, we avoid having to add new variables to memory in every round.

Also, we mentioned in~\autoref{subsec:approach-implementation} that, every round after the first, runs the same code as Curdleproofs.
Thus, only the first round should be able to introduce some computational overhead.
But, as mentioned before, the overhead should be negligible.

The same kind of explanation can be used to describe the same scenario at powers of two on the verifier side.
Looking at~\autoref{lst:ipa-verifier-optimized}(b), we see that the only difference in computation between CAAUrdleproofs and Curdleproofs stems from the calculation of $\mathbf{s}$.
Comparing line 29 and lines 33--47, it becomes clear that both ways work in $\mathcal{O}(n\log n)$, as they do $m$ computations for each of the $n$ elements.
CAAUrdleproofs does, however, need some more integer variables for splitting as well as an array for keeping track of the vector elements' active positions during recursion.
Nevertheless,~\autoref{fig:resulttimes}(b) shows that this does not have a big, if any, impact on the running time.

However, as mentioned in~\autoref{subsec:results:provingverifying}, when~$\ell$ is just above a power of two, we see some more aggressively increasing verifying times.
We expect this to be a result of~$m$ being set to~$\lceil\log n\rceil$ in line 8 of~\autoref{lst:ipa-verifier-optimized}.
E.g., when~$\ell$ is~$65$, it will have to handle computations from an additional recursive round, in comparison to when~$\ell$ is~$64$.
Additionally, this also explains why the increase in running time flattens when~$\ell$ is increases.

Though the pattern shows that this bump has a decreasing impact on the running time, the higher $\ell$ is, as mentioned in~\autoref{subsec:results:provingverifying}.
In theory, the addition of an extra proof element should introduce a constant amount of work for the verifier.

Therefore, we believe this to be an artifact of memory optimizations done either by hardware or Rust.
This could, for instance, include pre-fetching, in which the memory system can optimize access if it suspects some values in memory are going to be used some time later on\footnote{\href{https://doc.rust-lang.org/core/arch/aarch64/fn._prefetch.html}{https://doc.rust-lang.org/core/arch/aarc.h64/fn.\_prefetch.html} â€” Accessed: 29/05/2025}.
As $\ell$ increases, the memory system will have more data to predict and optimize memory access.

\subsection{Shuffle Security}\label{subsec:Discution-Shuffle-security}
When looking at the results of the shuffle security experiment in \autoref{fig:shufflesecurity} and \autoref{fig:shufflesecurityviolin}, we can see that when taking into account the standard deviation, the shuffle can still be secure with an~$\ell$ as low as 32 within the 8192 shuffles available.
Even when taking into account the worst case scenario from our experiment, the shuffle will still be secure with an~$\ell$ as low as 42 within the 8192 shuffles available with an $\alpha$ of 8192.

We would however not recommend using an~$\ell$ lower than 80, as here the worst case scenario needs a little under half the available shuffles to be honest in order to be secure.
As seen in~\autoref{fig:shufflesecurity} you would also only need a third of the 8192 shuffles to be honest to get within the one standard deviation.
This would still lead to a reduction of the proving time of 62.69ms, which is 74.25\% of the current Curdleproofs time and a reduction in the verifying time of 0.89ms, which is 96.11\% of the current Curdleproofs time.
It would also reduce the size of the block overhead from 16.656KB to 12.048KB.
Only 72.33\% of the currently calculated size for Curdleproofs.
This would result in saving $\sim 12.11GB$ of space on the blockchain each year.
Some other things to keep in mind when deciding on how many honest shuffles should be necessary to make the shuffle secure is that there are other factors that can affect the security of the blockchain.
One of such factors is some of the known attacks that takes advantage of controlling a large number of validators.
Attacks like the $\geq50\%$ stake attack and the $33\%$ finality attack~\cite{EthereumAttackDefense2024} takes advantage of controlling a large number of validators in order to negatively effect the blockchain system.
Because of attacks like these, which rely on controlling a large number of validators, we would recommend, when evaluating how many honest shuffles are necessary to make the shuffle secure; one should also take into account how many honest validators are necessary to make the blockchain secure.

Another thing to keep in mind is that within the Ethereum system not every validator is owned by a different person.
Some nodes contain multiple validators, and this means that during the shuffling phase, when selecting the 16384 possible proposers, there is a chance that a single node controls multiple of the chosen validators.
This is also possible during the selecting of the shufflers.

From the results we see that the mean starts higher and ends lower for the experiments with a higher $\alpha$.
One of the reasons for this could be the relationship between the number of adversarial tracked cups and the threshold necessary before the shuffle is secure.
Since the threshold is $2/(n-\alpha)$, the higher $\alpha$ is, the higher the threshold for the amount of water allowed in any cup.
Therefore, the higher $\alpha$ is, the harder it is to get the water divided into the honest cups.
The reason being, that the distribution only happens in honest cups. 
More adversarial cups means less honest cups to distribute the water into.
Hence, there potentially is a higher amount of water in the chosen cups after a shuffle when $\alpha$ is higher. 
